{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_and_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, trainer, loss_fn, dataloader, device):\n",
    "    \"\"\"\n",
    "    train一个epoch\n",
    "    return: 平均loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        trainer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        l = loss_fn(y_hat, y)\n",
    "        loss_sum += l.item()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    return loss_sum / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, loss_fn, dataloader, device):\n",
    "    \"\"\"\n",
    "    eval一个epoch\n",
    "    return: 评估指标\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    # loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = model(X)\n",
    "            # acc\n",
    "            preds = y_hat.argmax(1)\n",
    "\t        accuracy = torch.mean((preds == y).float()).item()\n",
    "            \"\"\"\n",
    "            # loss\n",
    "            l = loss_fn(y_hat, y)\n",
    "            loss_sum += l.item()\n",
    "            \"\"\"\n",
    "    return correct / len(dataloader.dataset)\n",
    "    # return loss_sum / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(epoches, model, trainer, loss_fn, \n",
    "                    train_dataloader, val_dataloader, device):\n",
    "    \"\"\"\n",
    "    训练并评估, 每个epoch记录一次metric\n",
    "    \"\"\"\n",
    "    loss_list, acc_list = [], []\n",
    "    for epoch in tqdm(range(epoches)):\n",
    "        loss_list.append(\n",
    "            train_epoch(model, trainer, loss_fn, train_dataloader, device))\n",
    "        acc_list.append(\n",
    "            evaluate_epoch(model, loss_fn, val_dataloader, device))\n",
    "    print(f'loss:{loss_list[-1]:.4f} acc:{acc_list[-1]:.4f}')\n",
    "    plt.plot(list(range(epoches)), loss_list, label='loss')\n",
    "    plt.plot(list(range(epoches)), acc_list, label='acc')\n",
    "    plt.legend(fontsize='small', loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_batch(batch, model, loss_fn, device):\n",
    "\t\"\"\"\n",
    "    forward一个batch\n",
    "    return: metric\n",
    "    \"\"\"\n",
    "\tX, y = batch\n",
    "\tX, y = X.to(device), y.to(device)\n",
    "\ty_hat = model(X)\n",
    "\tl = loss_fn(y_hat, y)\n",
    "\t\n",
    "\tpreds = y_hat.argmax(1)\n",
    "\taccuracy = torch.mean((preds == y).float())\n",
    "\t\n",
    "\treturn l, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(total_steps, valid_steps, \n",
    "\t\t\t\t\tmodel, optimizer, loss_fn, scheduler, \n",
    "\t\t\t\t\ttrain_dataloader, val_dataloader, device):\n",
    "    \"\"\"\n",
    "\ttotal_steps 约等于 len(dataloader) * epochs\n",
    "\t\"\"\"\n",
    "\ttrain_iter = iter(train_dataloader)\n",
    "    for step in range(total_steps):\n",
    "\t\t# 读一个batch的数据\n",
    "\t\ttry:\n",
    "\t\t\tbatch = next(train_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\ttrain_iter = iter(train_dataloader)\n",
    "\t\t\tbatch = next(train_iter)\n",
    "\n",
    "\t\tl, acc = forward_batch(batch, model, loss_fn, device)\n",
    "\t\tbatch_loss = l.item()\n",
    "\t\tbatch_accuracy = acc.item()\n",
    "\n",
    "\t\tl.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# scheduler.step() # optional\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "        if (step + 1) % valid_steps == 0:\n",
    "\t\t\tvalid_accuracy = val_epoch(val_dataloader, model, loss_fn, device)\n",
    "\t\t\t# 可添加画图和输出可视化metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### val_epoch使用forward_batch简洁代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, trainer, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "\trunning_loss, running_accuracy = 0.0, 0.0\n",
    "    for batch in dataloader:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tl, accuracy = forward_batch(batch, model, loss_fn, device)\n",
    "\t\t\trunning_loss += l.item()\n",
    "\t\t\trunning_accuracy += accuracy.item()\n",
    "    return running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr scheduler 按照不同类型的scheduler在epoch或batch后进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带warmup的learning rate随余弦函数变化的scheduler\n",
    "- Warmup Phase lr从0线性增加init_lr\n",
    "- Decay Phase lr随cos逐步降低到0\n",
    "  - 通过调整num_cycles决定经过cos的多少个周期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "\toptimizer: Optimizer,\n",
    "\tnum_warmup_steps: int,\n",
    "\tnum_training_steps: int,\n",
    "\tnum_cycles: float = 0.5, # cos的半个周期 lr降到0后不再上升\n",
    "\tlast_epoch: int = -1, # 表示从第0个epoch开始\n",
    "):\n",
    "\tdef lr_lambda(current_step):\n",
    "\t\t# Warmup\n",
    "\t\tif current_step < num_warmup_steps:\n",
    "\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n",
    "\t\t# decadence\n",
    "\t\tprogress = float(current_step - num_warmup_steps) / float(\n",
    "\t\t\tmax(1, num_training_steps - num_warmup_steps)\n",
    "\t\t)\n",
    "\t\treturn max(\n",
    "\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "\t\t)\n",
    "\n",
    "\treturn LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
